{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53857aa2ce24d1a0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f2a4f6da054a0b70",
   "metadata": {
    "collapsed": false,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-20T13:54:53.987768600Z",
     "start_time": "2024-02-20T13:54:52.905528600Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from speechbrain.pretrained import SpectralMaskEnhancement\n",
    "\n",
    "mask_model = SpectralMaskEnhancement.from_hparams(\n",
    "    source=\"speechbrain/metricgan-plus-voicebank\",\n",
    "    savedir=\"pretrained_models/metricgan-plus-voicebank\",\n",
    ")\n",
    "\n",
    "# Load and add fake batch dimension\n",
    "noisy = mask_model.load_audio(\n",
    "    \"speechbrain/metricgan-plus-voicebank/example.wav\"\n",
    ").unsqueeze(0)\n",
    "\n",
    "# Add relative length tensor\n",
    "enhanced = mask_model.enhance_batch(noisy, lengths=torch.tensor([1.]))\n",
    "\n",
    "# Saving enhanced signal on disk\n",
    "torchaudio.save('enhanced.wav', enhanced.cpu(), 16000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366e86b52b897049",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a493ee8f69a6218",
   "metadata": {
    "collapsed": false,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-20T12:56:49.031263800Z",
     "start_time": "2024-02-20T12:36:57.496070600Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPermissionError\u001B[0m                           Traceback (most recent call last)",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\tempfile.py:256\u001B[0m, in \u001B[0;36m_mkstemp_inner\u001B[1;34m(dir, pre, suf, flags, output_type)\u001B[0m\n\u001B[0;32m    255\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 256\u001B[0m     fd \u001B[38;5;241m=\u001B[39m _os\u001B[38;5;241m.\u001B[39mopen(file, flags, \u001B[38;5;241m0o600\u001B[39m)\n\u001B[0;32m    257\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mFileExistsError\u001B[39;00m:\n",
      "\u001B[1;31mPermissionError\u001B[0m: [Errno 13] Permission denied: 'C:\\\\ProgramData\\\\anaconda3\\\\Lib\\\\site-packages\\\\librosa\\\\core\\\\__pycache__\\\\tmp2vy_hblj'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 17\u001B[0m\n\u001B[0;32m     15\u001B[0m file_path_noisy \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(noisy_folder, file_name)\n\u001B[0;32m     16\u001B[0m file_path_denoised \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(denoised_folder, file_name)\n\u001B[1;32m---> 17\u001B[0m length \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(librosa\u001B[38;5;241m.\u001B[39mget_duration(path\u001B[38;5;241m=\u001B[39mfile_path_noisy) \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m)\n\u001B[0;32m     19\u001B[0m data_entry \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     20\u001B[0m     \n\u001B[0;32m     21\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnoisy_path\u001B[39m\u001B[38;5;124m'\u001B[39m: file_path_noisy,\n\u001B[0;32m     22\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdenoised_path\u001B[39m\u001B[38;5;124m'\u001B[39m: file_path_denoised,\n\u001B[0;32m     23\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlength\u001B[39m\u001B[38;5;124m'\u001B[39m: length\n\u001B[0;32m     24\u001B[0m }\n\u001B[0;32m     26\u001B[0m dataset[file_name] \u001B[38;5;241m=\u001B[39m data_entry\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\lazy_loader\\__init__.py:77\u001B[0m, in \u001B[0;36mattach.<locals>.__getattr__\u001B[1;34m(name)\u001B[0m\n\u001B[0;32m     75\u001B[0m submod_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpackage_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mattr_to_modules[name]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     76\u001B[0m submod \u001B[38;5;241m=\u001B[39m importlib\u001B[38;5;241m.\u001B[39mimport_module(submod_path)\n\u001B[1;32m---> 77\u001B[0m attr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(submod, name)\n\u001B[0;32m     79\u001B[0m \u001B[38;5;66;03m# If the attribute lives in a file (module) with the same\u001B[39;00m\n\u001B[0;32m     80\u001B[0m \u001B[38;5;66;03m# name as the attribute, ensure that the attribute and *not*\u001B[39;00m\n\u001B[0;32m     81\u001B[0m \u001B[38;5;66;03m# the module is accessible on the package.\u001B[39;00m\n\u001B[0;32m     82\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;241m==\u001B[39m attr_to_modules[name]:\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\lazy_loader\\__init__.py:76\u001B[0m, in \u001B[0;36mattach.<locals>.__getattr__\u001B[1;34m(name)\u001B[0m\n\u001B[0;32m     74\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m attr_to_modules:\n\u001B[0;32m     75\u001B[0m     submod_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpackage_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mattr_to_modules[name]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m---> 76\u001B[0m     submod \u001B[38;5;241m=\u001B[39m importlib\u001B[38;5;241m.\u001B[39mimport_module(submod_path)\n\u001B[0;32m     77\u001B[0m     attr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(submod, name)\n\u001B[0;32m     79\u001B[0m     \u001B[38;5;66;03m# If the attribute lives in a file (module) with the same\u001B[39;00m\n\u001B[0;32m     80\u001B[0m     \u001B[38;5;66;03m# name as the attribute, ensure that the attribute and *not*\u001B[39;00m\n\u001B[0;32m     81\u001B[0m     \u001B[38;5;66;03m# the module is accessible on the package.\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\importlib\\__init__.py:126\u001B[0m, in \u001B[0;36mimport_module\u001B[1;34m(name, package)\u001B[0m\n\u001B[0;32m    124\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m    125\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m--> 126\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _bootstrap\u001B[38;5;241m.\u001B[39m_gcd_import(name[level:], package, level)\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1204\u001B[0m, in \u001B[0;36m_gcd_import\u001B[1;34m(name, package, level)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1176\u001B[0m, in \u001B[0;36m_find_and_load\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1147\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:690\u001B[0m, in \u001B[0;36m_load_unlocked\u001B[1;34m(spec)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:940\u001B[0m, in \u001B[0;36mexec_module\u001B[1;34m(self, module)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:241\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[1;34m(f, *args, **kwds)\u001B[0m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\librosa\\core\\audio.py:19\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumba\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m jit, stencil, guvectorize\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfft\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_fftlib\n\u001B[1;32m---> 19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconvert\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m frames_to_samples, time_to_samples\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_cache\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m cache\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m util\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\librosa\\core\\convert.py:7\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mre\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m notation\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexceptions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ParameterError\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdecorators\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m vectorize\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\librosa\\core\\notation.py:796\u001B[0m\n\u001B[0;32m    789\u001B[0m     acc_str \u001B[38;5;241m=\u001B[39m acc_map_inv[np\u001B[38;5;241m.\u001B[39msign(acc_index) \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m] \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mint\u001B[39m(\n\u001B[0;32m    790\u001B[0m         \u001B[38;5;28mabs\u001B[39m(acc_index) \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[0;32m    791\u001B[0m     ) \u001B[38;5;241m+\u001B[39m acc_map_inv[np\u001B[38;5;241m.\u001B[39msign(acc_index)] \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mint\u001B[39m(\u001B[38;5;28mabs\u001B[39m(acc_index) \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m    793\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m raw_output \u001B[38;5;241m+\u001B[39m acc_str\n\u001B[1;32m--> 796\u001B[0m \u001B[38;5;129m@jit\u001B[39m(nopython\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, nogil\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, cache\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    797\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__o_fold\u001B[39m(d):\n\u001B[0;32m    798\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Compute the octave-folded interval.\u001B[39;00m\n\u001B[0;32m    799\u001B[0m \n\u001B[0;32m    800\u001B[0m \u001B[38;5;124;03m    This maps intervals to the range [1, 2).\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    804\u001B[0m \u001B[38;5;124;03m    documentation.\u001B[39;00m\n\u001B[0;32m    805\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m    806\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m d \u001B[38;5;241m*\u001B[39m (\u001B[38;5;241m2.0\u001B[39m \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m-\u001B[39mnp\u001B[38;5;241m.\u001B[39mfloor(np\u001B[38;5;241m.\u001B[39mlog2(d)))\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\numba\\core\\decorators.py:234\u001B[0m, in \u001B[0;36m_jit.<locals>.wrapper\u001B[1;34m(func)\u001B[0m\n\u001B[0;32m    230\u001B[0m disp \u001B[38;5;241m=\u001B[39m dispatcher(py_func\u001B[38;5;241m=\u001B[39mfunc, \u001B[38;5;28mlocals\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlocals\u001B[39m,\n\u001B[0;32m    231\u001B[0m                   targetoptions\u001B[38;5;241m=\u001B[39mtargetoptions,\n\u001B[0;32m    232\u001B[0m                   \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdispatcher_args)\n\u001B[0;32m    233\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cache:\n\u001B[1;32m--> 234\u001B[0m     disp\u001B[38;5;241m.\u001B[39menable_caching()\n\u001B[0;32m    235\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sigs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    236\u001B[0m     \u001B[38;5;66;03m# Register the Dispatcher to the type inference mechanism,\u001B[39;00m\n\u001B[0;32m    237\u001B[0m     \u001B[38;5;66;03m# even though the decorator hasn't returned yet.\u001B[39;00m\n\u001B[0;32m    238\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumba\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m typeinfer\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\numba\\core\\dispatcher.py:863\u001B[0m, in \u001B[0;36mDispatcher.enable_caching\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    862\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21menable_caching\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m--> 863\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cache \u001B[38;5;241m=\u001B[39m FunctionCache(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpy_func)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\numba\\core\\caching.py:601\u001B[0m, in \u001B[0;36mCache.__init__\u001B[1;34m(self, py_func)\u001B[0m\n\u001B[0;32m    599\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mrepr\u001B[39m(py_func)\n\u001B[0;32m    600\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_py_func \u001B[38;5;241m=\u001B[39m py_func\n\u001B[1;32m--> 601\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_impl \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_impl_class(py_func)\n\u001B[0;32m    602\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cache_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_impl\u001B[38;5;241m.\u001B[39mlocator\u001B[38;5;241m.\u001B[39mget_cache_path()\n\u001B[0;32m    603\u001B[0m \u001B[38;5;66;03m# This may be a bit strict but avoids us maintaining a magic number\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\numba\\core\\caching.py:333\u001B[0m, in \u001B[0;36mCacheImpl.__init__\u001B[1;34m(self, py_func)\u001B[0m\n\u001B[0;32m    331\u001B[0m source_path \u001B[38;5;241m=\u001B[39m inspect\u001B[38;5;241m.\u001B[39mgetfile(py_func)\n\u001B[0;32m    332\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_locator_classes:\n\u001B[1;32m--> 333\u001B[0m     locator \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mfrom_function(py_func, source_path)\n\u001B[0;32m    334\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m locator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    335\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\numba\\core\\caching.py:180\u001B[0m, in \u001B[0;36m_SourceFileBackedLocatorMixin.from_function\u001B[1;34m(cls, py_func, py_file)\u001B[0m\n\u001B[0;32m    178\u001B[0m \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m(py_func, py_file)\n\u001B[0;32m    179\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 180\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mensure_cache_path()\n\u001B[0;32m    181\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m:\n\u001B[0;32m    182\u001B[0m     \u001B[38;5;66;03m# Cannot ensure the cache directory exists or is writable\u001B[39;00m\n\u001B[0;32m    183\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\numba\\core\\caching.py:107\u001B[0m, in \u001B[0;36m_CacheLocator.ensure_cache_path\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    105\u001B[0m os\u001B[38;5;241m.\u001B[39mmakedirs(path, exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    106\u001B[0m \u001B[38;5;66;03m# Ensure the directory is writable by trying to write a temporary file\u001B[39;00m\n\u001B[1;32m--> 107\u001B[0m tempfile\u001B[38;5;241m.\u001B[39mTemporaryFile(\u001B[38;5;28mdir\u001B[39m\u001B[38;5;241m=\u001B[39mpath)\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\tempfile.py:563\u001B[0m, in \u001B[0;36mNamedTemporaryFile\u001B[1;34m(mode, buffering, encoding, newline, suffix, prefix, dir, delete, errors)\u001B[0m\n\u001B[0;32m    561\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fd\n\u001B[0;32m    562\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 563\u001B[0m     file \u001B[38;5;241m=\u001B[39m _io\u001B[38;5;241m.\u001B[39mopen(\u001B[38;5;28mdir\u001B[39m, mode, buffering\u001B[38;5;241m=\u001B[39mbuffering,\n\u001B[0;32m    564\u001B[0m                     newline\u001B[38;5;241m=\u001B[39mnewline, encoding\u001B[38;5;241m=\u001B[39mencoding, errors\u001B[38;5;241m=\u001B[39merrors,\n\u001B[0;32m    565\u001B[0m                     opener\u001B[38;5;241m=\u001B[39mopener)\n\u001B[0;32m    566\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    567\u001B[0m         raw \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(file, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbuffer\u001B[39m\u001B[38;5;124m'\u001B[39m, file)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\tempfile.py:560\u001B[0m, in \u001B[0;36mNamedTemporaryFile.<locals>.opener\u001B[1;34m(*args)\u001B[0m\n\u001B[0;32m    558\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mopener\u001B[39m(\u001B[38;5;241m*\u001B[39margs):\n\u001B[0;32m    559\u001B[0m     \u001B[38;5;28;01mnonlocal\u001B[39;00m name\n\u001B[1;32m--> 560\u001B[0m     fd, name \u001B[38;5;241m=\u001B[39m _mkstemp_inner(\u001B[38;5;28mdir\u001B[39m, prefix, suffix, flags, output_type)\n\u001B[0;32m    561\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fd\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\tempfile.py:262\u001B[0m, in \u001B[0;36m_mkstemp_inner\u001B[1;34m(dir, pre, suf, flags, output_type)\u001B[0m\n\u001B[0;32m    258\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m    \u001B[38;5;66;03m# try again\u001B[39;00m\n\u001B[0;32m    259\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mPermissionError\u001B[39;00m:\n\u001B[0;32m    260\u001B[0m     \u001B[38;5;66;03m# This exception is thrown when a directory with the chosen name\u001B[39;00m\n\u001B[0;32m    261\u001B[0m     \u001B[38;5;66;03m# already exists on windows.\u001B[39;00m\n\u001B[1;32m--> 262\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (_os\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnt\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m _os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misdir(\u001B[38;5;28mdir\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[0;32m    263\u001B[0m         _os\u001B[38;5;241m.\u001B[39maccess(\u001B[38;5;28mdir\u001B[39m, _os\u001B[38;5;241m.\u001B[39mW_OK)):\n\u001B[0;32m    264\u001B[0m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m    265\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m<frozen genericpath>:39\u001B[0m, in \u001B[0;36misdir\u001B[1;34m(s)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import librosa\n",
    "\n",
    "\n",
    "noisy_folder = 'C:/Users/vesha/PycharmProjects/pC/HOH/noisy'\n",
    "denoised_folder = 'C:/Users/vesha/PycharmProjects/pC/HOH/clean'\n",
    "output_file = 'C:/Users/vesha/PycharmProjects/pC/output.json'\n",
    "\n",
    "dataset = {}\n",
    "\n",
    "# Iterate over the files in the noisy folder\n",
    "for file_name in os.listdir(noisy_folder):\n",
    "    if file_name.endswith('.wav'):  # Adjust the file extension if necessary\n",
    "        file_path_noisy = os.path.join(noisy_folder, file_name)\n",
    "        file_path_denoised = os.path.join(denoised_folder, file_name)\n",
    "        length = int(librosa.get_duration(path=file_path_noisy) * 1000)\n",
    "\n",
    "        data_entry = {\n",
    "            \n",
    "            'noisy_path': file_path_noisy,\n",
    "            'denoised_path': file_path_denoised,\n",
    "            'length': length\n",
    "        }\n",
    "\n",
    "        dataset[file_name] = data_entry\n",
    "\n",
    "# Save the dataset as a JSON file\n",
    "with open(output_file, 'w') as json_file:\n",
    "    json.dump(dataset, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "import speechbrain\n",
    "import torch\n",
    "from speechbrain.dataio.dataset import DynamicItemDataset\n",
    "\n",
    "dataset_test = DynamicItemDataset.from_json(\"output2.json\")\n",
    "\n",
    "@speechbrain.utils.data_pipeline.takes(\"noisy_path\")\n",
    "@speechbrain.utils.data_pipeline.provides(\"noisy_signal\", \"enhance\")\n",
    "def audio_pipeline_noisy(noisy_path):\n",
    "      noisy_signal = mask_model.load_audio(noisy_path).unsqueeze(0)\n",
    "      enhance = mask_model.enhance_batch(noisy_signal, lengths=torch.tensor([1.]))\n",
    "      return noisy_signal, enhance\n",
    "@speechbrain.utils.data_pipeline.takes(\"denoised_path\")\n",
    "@speechbrain.utils.data_pipeline.provides(\"clean_signal\")\n",
    "def audio_pipeline_clean(denoised_path):\n",
    "        clean_signal = mask_model.load_audio(denoised_path).unsqueeze(0)\n",
    "        return clean_signal"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T13:55:31.681519500Z",
     "start_time": "2024-02-20T13:55:31.654291800Z"
    }
   },
   "id": "7c247bd030b9d241"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 6.1155e-04,  3.4434e-03, -8.1838e-07,  ...,  1.0411e-01,\n          9.9643e-02,  9.0381e-02]])"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test.add_dynamic_item(audio_pipeline_noisy)\n",
    "dataset_test.add_dynamic_item(audio_pipeline_clean)\n",
    "dataset_test.set_output_keys([\"id\", \"noisy_signal\", \"clean_signal\", \"enhance\"])\n",
    "dataset_test[0][\"enhance\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T13:55:34.317671700Z",
     "start_time": "2024-02-20T13:55:33.192544800Z"
    }
   },
   "id": "839c78ffabbb38d5"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "8"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_test)\n",
    "#print(dataset[0][\"noisy_signal\"].size(), dataset[0][\"clean_signal\"].size())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T13:55:42.460852100Z",
     "start_time": "2024-02-20T13:55:42.425580500Z"
    }
   },
   "id": "345c93e2964f2a52"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 29916])\n",
      "torch.Size([1, 29916])\n",
      "torch.Size([1, 29916])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(dataset_test[0][\"noisy_signal\"].size())\n",
    "print(dataset_test[0][\"clean_signal\"].size())\n",
    "print(dataset_test[0][\"enhance\"].size())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T13:56:08.466390900Z",
     "start_time": "2024-02-20T13:56:04.670563100Z"
    }
   },
   "id": "da73a802e7c61049"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2d11c972b9040ea0"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m pesqs\u001B[38;5;241m=\u001B[39m[]\n\u001B[0;32m      3\u001B[0m output_file \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mC:/Users/vesha/PycharmProjects//logs/origin_pesq.txt\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m dataset_test:\n\u001B[0;32m      5\u001B[0m     clean_signal \u001B[38;5;241m=\u001B[39mi[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclean_signal\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mnumpy()\u001B[38;5;241m.\u001B[39msqueeze()\n\u001B[0;32m      6\u001B[0m     denoised_signal \u001B[38;5;241m=\u001B[39m i[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menhance\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mnumpy()\u001B[38;5;241m.\u001B[39msqueeze()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'dataset_test' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import pesq\n",
    "import numpy as np\n",
    "clean_signal = dataset_test[0][\"clean_signal\"].numpy().squeeze()\n",
    "#noisy_signal = noisy_signal.numpy()\n",
    "denoised_signal = dataset_test[0][\"enhance\"].numpy().squeeze()\n",
    "print(denoised_signal.shape)\n",
    "pesq_score = pesq.pesq(16000, denoised_signal, clean_signal, 'wb')\n",
    "print(pesq_score)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T16:36:28.948555300Z",
     "start_time": "2024-02-20T16:36:28.136847100Z"
    }
   },
   "id": "19b3dbb71781918a"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "import pesq"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T13:56:32.880696100Z",
     "start_time": "2024-02-20T13:56:32.851166800Z"
    }
   },
   "id": "b5bb8a36aa140644"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baa0b1e202fe41ce",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T16:48:07.265173200Z",
     "start_time": "2024-02-20T16:48:07.236686800Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m pesqs\u001B[38;5;241m=\u001B[39m[]\n\u001B[0;32m      4\u001B[0m output_file \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mC:/Users/vesha/PycharmProjects//logs/origin_pesq.txt\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m dataset_test:\n\u001B[0;32m      6\u001B[0m     clean_signal \u001B[38;5;241m=\u001B[39mi[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclean_signal\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mnumpy()\u001B[38;5;241m.\u001B[39msqueeze()\n\u001B[0;32m      7\u001B[0m     denoised_signal \u001B[38;5;241m=\u001B[39m i[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menhance\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mnumpy()\u001B[38;5;241m.\u001B[39msqueeze()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'dataset_test' is not defined"
     ]
    }
   ],
   "source": [
    "total_pesq = 0\n",
    "pesqs=[]\n",
    "output_file = 'C:/Users/vesha/PycharmProjects//logs/origin_pesq.txt'\n",
    "for i in dataset_test:\n",
    "    clean_signal =i[\"clean_signal\"].numpy().squeeze()\n",
    "    denoised_signal = i[\"enhance\"].numpy().squeeze()\n",
    "    pesq_score = pesq.pesq(16000, clean_signal, denoised_signal, 'wb')\n",
    "    pesqs.append(pesq_score)\n",
    "    total_pesq=pesq_score+total_pesq\n",
    "    print(pesq_score)\n",
    "    \n",
    "average_pesq = total_pesq / len(dataset_test)\n",
    "print(f\"Average pesq: {average_pesq}\")\n",
    "\n",
    "with open(output_file, 'w') as file:\n",
    "    for score in pesqs:\n",
    "        file.write(str(score) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "odict_keys(['enhance_model'])"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_model.mods.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T14:21:18.230373200Z",
     "start_time": "2024-02-20T14:21:18.188104200Z"
    }
   },
   "id": "f0267caca89671ef"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "EnhancementGenerator(\n  (activation): LeakyReLU(negative_slope=0.3)\n  (blstm): LSTM(\n    (rnn): LSTM(257, 200, num_layers=2, batch_first=True, bidirectional=True)\n  )\n  (linear1): Linear(in_features=400, out_features=300, bias=True)\n  (linear2): Linear(in_features=300, out_features=257, bias=True)\n  (Learnable_sigmoid): Learnable_sigmoid()\n  (sigmoid): Sigmoid()\n)"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_model.mods.enhance_model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T14:21:18.786598900Z",
     "start_time": "2024-02-20T14:21:18.713256600Z"
    }
   },
   "id": "fdd603a823872222"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "['__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n 'compute_istft',\n 'compute_stft',\n 'enhance_model',\n 'hop_length',\n 'modules',\n 'n_fft',\n 'pretrainer',\n 'resynth',\n 'sample_rate',\n 'spectral_magnitude',\n 'win_length',\n 'window_fn']"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(mask_model.hparams)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T14:21:19.614416500Z",
     "start_time": "2024-02-20T14:21:19.577447600Z"
    }
   },
   "id": "a5521a4e16dedfd9"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "ModuleDict(\n  (enhance_model): EnhancementGenerator(\n    (activation): LeakyReLU(negative_slope=0.3)\n    (blstm): LSTM(\n      (rnn): LSTM(257, 200, num_layers=2, batch_first=True, bidirectional=True)\n    )\n    (linear1): Linear(in_features=400, out_features=300, bias=True)\n    (linear2): Linear(in_features=300, out_features=257, bias=True)\n    (Learnable_sigmoid): Learnable_sigmoid()\n    (sigmoid): Sigmoid()\n  )\n)"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_model.mods"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T14:21:20.509261700Z",
     "start_time": "2024-02-20T14:21:20.482547200Z"
    }
   },
   "id": "1277e8afb6b24bcc"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "import speechbrain\n",
    "import torch\n",
    "from speechbrain.dataio.dataset import DynamicItemDataset\n",
    "\n",
    "dataset_train = DynamicItemDataset.from_json(\"output1.json\")\n",
    "dataset_test = DynamicItemDataset.from_json(\"output2.json\")\n",
    "#dataset_test = DynamicItemDataset.from_json(\"output3.json\")\n",
    "@speechbrain.utils.data_pipeline.takes(\"noisy_path\",\"denoised_path\" )\n",
    "@speechbrain.utils.data_pipeline.provides(\"signal\", \"enhanced_signal\")\n",
    "def audio_pipeline(noisy_path,denoised_path):\n",
    "    signal = mask_model.load_audio(noisy_path)\n",
    "    yield signal\n",
    "    enhanced_signal = mask_model.load_audio(denoised_path)\n",
    "    yield enhanced_signal\n",
    "      "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T14:21:23.620368700Z",
     "start_time": "2024-02-20T14:21:23.581993800Z"
    }
   },
   "id": "9e14a8dfff8eb5f7"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "dataset_train.add_dynamic_item(audio_pipeline)\n",
    "dataset_train.set_output_keys([\"id\", \"signal\", \"enhanced_signal\"])\n",
    "\n",
    "dataset_test.add_dynamic_item(audio_pipeline)\n",
    "dataset_test.set_output_keys([\"id\", \"signal\", \"enhanced_signal\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T14:21:25.453655600Z",
     "start_time": "2024-02-20T14:21:25.449623500Z"
    }
   },
   "id": "5eaa7f8dfc20add7"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 56\n",
      "torch.Size([29916])\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_test), len(dataset_train))\n",
    "print(dataset_test[0][\"enhanced_signal\"].size())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T14:21:28.436557400Z",
     "start_time": "2024-02-20T14:21:27.342679600Z"
    }
   },
   "id": "7dba467bc920ef5"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "from speechbrain.processing.features import spectral_magnitude\n",
    "from speechbrain.processing.signal_processing import resynthesize\n",
    "import librosa\n",
    "import  speechbrain as sb\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pesq\n",
    "class MaskEnhancementFineTune(sb.Brain):\n",
    "    def on_stage_start(self, stage, epoch):\n",
    "        # enable grad for all modules we want to fine-tune\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            for module in [self.modules.mask_model, self.modules.compute_stft,self.modules.compute_istft ]:\n",
    "                for p in module.parameters():\n",
    "                    p.requires_grad = True\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            self.train_loss_history = []\n",
    "            self.train_pesq_history = []\n",
    "        if stage == sb.Stage.TEST:\n",
    "            self.test_loss_history = []\n",
    "            self.test_pesq_history = []\n",
    "        \n",
    "            \n",
    "            \n",
    "    def compute_feats(self, wavs):\n",
    "        \"\"\"Feature computation pipeline\"\"\"\n",
    "        feats = self.modules.compute_stft(wavs)\n",
    "        feats = spectral_magnitude(feats, power=0.7)\n",
    "        feats = torch.log1p(feats)\n",
    "        return feats\n",
    "    def compute_forward(self, batch, stage):\n",
    "        \"\"\"Forward computations from the input signal to the enhanced signal.\"\"\"\n",
    "        \n",
    "        batch = batch.to(self.device)\n",
    "        noisy_signal, noisy_lens = batch.signal\n",
    "        # # Forward pass\n",
    "        noisy_spec = self.compute_feats(noisy_signal)        \n",
    "        mask = self.modules.mask_model.mods.enhance_model(noisy_spec, noisy_lens)\n",
    "        predict_spec = torch.mul(mask, noisy_spec)\n",
    "        predict_wav = self.modules.mask_model.hparams.resynth(torch.expm1(predict_spec), noisy_signal)\n",
    "\n",
    "        return predict_wav\n",
    "    def on_stage_end(self, stage, stage_loss, epoch):\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            self.save_training_logs(\"logs/train\")\n",
    "        elif stage == sb.Stage.TEST:\n",
    "            self.save_validation_logs(\"logs/test\")\n",
    "        \n",
    "    def compute_objectives(self, predictions, batch, stage):\n",
    "        \"\"\"Computes the loss (MSE) given predictions and targets.\"\"\"\n",
    "       \n",
    "        predict_wav = predictions\n",
    "        predict_spec = self.compute_feats(predict_wav)\n",
    "        clean_wav, lens = batch.enhanced_signal\n",
    "        clean_spec = self.compute_feats(clean_wav)\n",
    "        # Adjust the shape of the enhanced signals and targets\n",
    "        loss = self.hparams.mse_loss(predict_spec, clean_spec)\n",
    "        predict_wav = predict_wav.detach().cpu().numpy()\n",
    "        clean_wav = clean_wav.detach().cpu().numpy()\n",
    "        pesq_score = np.mean([pesq.pesq(fs=16000, ref=clean_wav[i], deg=predict_wav[i], mode='wb') for i in range(len(clean_wav))])\n",
    "        print(pesq_score)\n",
    "        print(loss)\n",
    "        return loss, pesq_score\n",
    "    \n",
    "    def fit_batch(self, batch):\n",
    "        \n",
    "        \"\"\"Train the parameters given a single batch in input\"\"\"\n",
    "       \n",
    "        predictions = self.compute_forward(batch, \n",
    "              sb.Stage.TRAIN)\n",
    "\n",
    "        loss, pesq_score  = self.compute_objectives(predictions, batch, sb.Stage.TRAIN )\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        if self.check_gradients(loss):\n",
    "            self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "        self.train_loss_history.append(loss.detach())\n",
    "        self.train_pesq_history.append(pesq_score)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def evaluate_batch(self, batch, stage):\n",
    "        predictions = self.compute_forward(batch, \n",
    "              sb.Stage.TRAIN)\n",
    "\n",
    "        loss, pesq_score  = self.compute_objectives(predictions, batch, sb.Stage.TRAIN )\n",
    "        self.test_loss_history.append(loss.detach())\n",
    "        self.test_pesq_history.append(pesq_score)\n",
    "        return loss\n",
    "    \n",
    "        \n",
    "    def save_training_logs(self, logs_dir):\n",
    "        train_loss_path = os.path.join(\"C:/Users/vesha/PycharmProjects/pC/logs\", \"train_loss.txt\")  \n",
    "        train_pesq_path = os.path.join(\"C:/Users/vesha/PycharmProjects/pC/logs\", \"train_pesq.txt\")\n",
    "        with open(train_loss_path, \"w\") as f:\n",
    "            f.write(\"\\n\".join(str(loss) for loss in self.train_loss_history))\n",
    "             \n",
    "        with open(train_pesq_path, \"w\") as f:\n",
    "            f.write(\"\\n\".join(str(pesq) for pesq in self.train_pesq_history))\n",
    "            \n",
    "    def save_validation_logs(self, logs_dir):\n",
    "        valid_loss_path = os.path.join(\"C:/Users/vesha/PycharmProjects/pC/logs\", \"val_loss.txt\", )\n",
    "        valid_pesq_path = os.path.join(\"C:/Users/vesha/PycharmProjects/pC/logs\", \"val_pesq.txt\")\n",
    "        with open(valid_loss_path, \"w\") as f:\n",
    "            f.write(\"\\n\".join(str(loss) for loss in self.test_loss_history))\n",
    "             \n",
    "        with open(valid_pesq_path, \"w\") as f:\n",
    "            f.write(\"\\n\".join(str(pesq) for pesq in self.test_pesq_history))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T14:21:32.422675700Z",
     "start_time": "2024-02-20T14:21:32.395833700Z"
    }
   },
   "id": "f164f2b8ac51d474"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "import pesq\n",
    "modules = {\"mask_model\": mask_model,\n",
    "           \"compute_stft\": mask_model.hparams.compute_stft,\n",
    "           \"compute_istft\": mask_model.hparams.compute_istft\n",
    "           }\n",
    "hparams = {\"mse_loss\": torch.nn.MSELoss(),\n",
    "           \"pesq\": pesq}\n",
    "MyModel = MaskEnhancementFineTune(modules, hparams=hparams, opt_class=lambda x: torch.optim.SGD(x, 1e-5))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T14:21:34.681612600Z",
     "start_time": "2024-02-20T14:21:34.634668800Z"
    }
   },
   "id": "31c14fa84bc9f970"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6561397314071655\n",
      "tensor(0.0201, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 1/18 [00:06<01:55,  6.77s/it, train_loss=0.0201]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2050107320149739\n",
      "tensor(0.0705, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 2/18 [00:11<01:27,  5.47s/it, train_loss=0.0453]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6141529480616252\n",
      "tensor(0.0292, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 3/18 [00:16<01:19,  5.32s/it, train_loss=0.0399]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4009600083033245\n",
      "tensor(0.0279, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 4/18 [00:22<01:19,  5.67s/it, train_loss=0.0369]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6452123324076335\n",
      "tensor(0.0203, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 5/18 [00:36<01:51,  8.54s/it, train_loss=0.0336]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5832857688268025\n",
      "tensor(0.0365, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 6/18 [00:40<01:25,  7.12s/it, train_loss=0.0341]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7570412556330364\n",
      "tensor(0.0338, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 7/18 [00:44<01:06,  6.08s/it, train_loss=0.034] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7323226531346638\n",
      "tensor(0.0305, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 8/18 [00:49<00:58,  5.83s/it, train_loss=0.0336]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2773929437001545\n",
      "tensor(0.0440, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 9/18 [00:54<00:48,  5.38s/it, train_loss=0.0347]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5704311927159627\n",
      "tensor(0.0284, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 10/18 [00:58<00:40,  5.03s/it, train_loss=0.0341]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7897350788116455\n",
      "tensor(0.0303, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 11/18 [01:01<00:31,  4.51s/it, train_loss=0.0338]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7454382578531902\n",
      "tensor(0.0223, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 12/18 [01:05<00:26,  4.36s/it, train_loss=0.0328]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.895368218421936\n",
      "tensor(0.0239, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 13/18 [01:10<00:22,  4.53s/it, train_loss=0.0321]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9185400009155273\n",
      "tensor(0.0275, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 14/18 [01:15<00:18,  4.72s/it, train_loss=0.0318]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9788245757420857\n",
      "tensor(0.0296, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 15/18 [01:21<00:14,  4.91s/it, train_loss=0.0316]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.91318941116333\n",
      "tensor(0.0199, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 16/18 [01:28<00:11,  5.64s/it, train_loss=0.0309]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.435445745786031\n",
      "tensor(0.0215, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 17/18 [01:35<00:06,  6.14s/it, train_loss=0.0304]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7011317412058513\n",
      "tensor(0.0310, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [01:45<00:00,  5.88s/it, train_loss=0.0304]\n",
      "  0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6561453342437744\n",
      "tensor(0.0201, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 1/18 [00:09<02:35,  9.16s/it, train_loss=0.0201]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2050151824951172\n",
      "tensor(0.0705, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 2/18 [00:16<02:08,  8.01s/it, train_loss=0.0453]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6141589482625325\n",
      "tensor(0.0292, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 3/18 [00:22<01:50,  7.37s/it, train_loss=0.0399]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4009619156519573\n",
      "tensor(0.0279, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 4/18 [00:34<02:07,  9.11s/it, train_loss=0.0369]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.645210901896159\n",
      "tensor(0.0203, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 5/18 [00:48<02:20, 10.81s/it, train_loss=0.0336]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5832861264546711\n",
      "tensor(0.0365, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 6/18 [00:52<01:39,  8.31s/it, train_loss=0.0341]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7570470968882244\n",
      "tensor(0.0338, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 7/18 [00:56<01:18,  7.13s/it, train_loss=0.034] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7323216199874878\n",
      "tensor(0.0305, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 8/18 [01:02<01:08,  6.83s/it, train_loss=0.0336]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.277393897374471\n",
      "tensor(0.0440, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 9/18 [01:08<00:57,  6.38s/it, train_loss=0.0347]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5704344908396404\n",
      "tensor(0.0284, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 10/18 [01:12<00:45,  5.74s/it, train_loss=0.0341]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7897495031356812\n",
      "tensor(0.0303, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 11/18 [01:15<00:34,  4.97s/it, train_loss=0.0338]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7454412778218586\n",
      "tensor(0.0223, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 12/18 [01:19<00:27,  4.60s/it, train_loss=0.0328]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8953773180643718\n",
      "tensor(0.0239, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 13/18 [01:25<00:24,  4.88s/it, train_loss=0.0321]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9185267289479573\n",
      "tensor(0.0275, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 14/18 [01:30<00:20,  5.03s/it, train_loss=0.0318]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9788331985473633\n",
      "tensor(0.0296, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 15/18 [01:35<00:15,  5.08s/it, train_loss=0.0316]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.913197676340739\n",
      "tensor(0.0199, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 16/18 [01:43<00:11,  5.86s/it, train_loss=0.0309]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4354498783747356\n",
      "tensor(0.0215, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 17/18 [01:50<00:06,  6.26s/it, train_loss=0.0304]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7011350393295288\n",
      "tensor(0.0310, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [01:56<00:00,  6.48s/it, train_loss=0.0304]\n"
     ]
    }
   ],
   "source": [
    "MyModel.fit(range(2),\n",
    "    #valid_set=dataset_test,\n",
    "    train_set=dataset_train,\n",
    "    train_loader_kwargs={\"batch_size\": 3, \"drop_last\": True, \"shuffle\": False},                     #valid_loader_kwargs={\"batch_size\": 1, \"drop_last\": True, \"shuffle\": False}\n",
    " )\n",
    "#print(eval_stats)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T14:25:25.319392700Z",
     "start_time": "2024-02-20T14:21:42.792086400Z"
    }
   },
   "id": "8e149e63da715889"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [00:00<00:06,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1754937171936035\n",
      "tensor(0.0455)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [00:02<00:08,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6018834114074707\n",
      "tensor(0.0298)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [00:04<00:07,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2756019830703735\n",
      "tensor(0.0466)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [00:05<00:05,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.03309965133667\n",
      "tensor(0.0753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 5/8 [00:07<00:04,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.249719262123108\n",
      "tensor(0.0614)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [00:08<00:02,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2876050472259521\n",
      "tensor(0.0676)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7/8 [00:09<00:01,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3024944067001343\n",
      "tensor(0.0795)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:10<00:00,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.140517234802246\n",
      "tensor(0.0948)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.06255585281178355"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyModel.evaluate(test_set= dataset_test,\n",
    "        test_loader_kwargs={\"batch_size\": 1, \"drop_last\": True, \"shuffle\": False})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T14:27:26.349284900Z",
     "start_time": "2024-02-20T14:27:16.135169700Z"
    }
   },
   "id": "5dd8ad42ed9e747a"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "import torch\n",
    "model = MaskEnhancementFineTune()\n",
    "# Save the model parameters\n",
    "torch.save(model, 'C:/Users/vesha/PycharmProjects/pC/model/FineTuned.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T14:27:52.882386200Z",
     "start_time": "2024-02-20T14:27:52.852324400Z"
    }
   },
   "id": "a9d3f4a68127300"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3daa7e30ed41390e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
